{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC Project : Predicting house prices "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Import of modules need for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import exp\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loading and printing data, one for train and another for test. houses data are based on houses.csv for training and houses_new on houses_competition.csv are test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
      "6681    3.500         3       2.25         1860      8378     2.0         0.0   \n",
      "17798   5.925         4       3.00         2170      8240     1.0         0.0   \n",
      "18854   2.555         2       1.00         1440     43560     1.0         0.0   \n",
      "13478  13.300         4       2.25         3260      4640     2.0         0.0   \n",
      "10509   3.891         2       1.00          840      5400     1.0         0.0   \n",
      "...       ...       ...        ...          ...       ...     ...         ...   \n",
      "16125   2.900         2       1.00          930      7740     1.0         0.0   \n",
      "19004   3.150         3       2.50         1730      6368     2.0         0.0   \n",
      "9094    6.850         3       2.50         3450      8000     3.0         0.0   \n",
      "3537    3.260         6       1.50         1930      8400     1.0         0.0   \n",
      "10054   3.150         2       2.25         1290      2436     2.0         0.0   \n",
      "\n",
      "       view  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
      "6681    0.0          3      7        1860              0      1995   \n",
      "17798   0.0          4      8        1370            800      1968   \n",
      "18854   0.0          4      7        1150            290      1965   \n",
      "13478   0.0          5      9        2360            900      1907   \n",
      "10509   0.0          4      7         840              0      1948   \n",
      "...     ...        ...    ...         ...            ...       ...   \n",
      "16125   0.0          3      6         930              0      1924   \n",
      "19004   0.0          3      7        1730              0      1993   \n",
      "9094    0.0          4      8        2970            480      1927   \n",
      "3537    0.0          3      7        1030            900      1971   \n",
      "10054   0.0          3      7        1290              0      1984   \n",
      "\n",
      "       yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
      "6681            0.0    98038  47.3875 -122.032           1870        8378  \n",
      "17798           0.0    98052  47.6291 -122.093           2020        7944  \n",
      "18854           0.0    98027  47.4916 -122.082           1870       56628  \n",
      "13478           0.0    98112  47.6272 -122.312           3240        5800  \n",
      "10509           0.0    98118  47.5489 -122.271           1340        5400  \n",
      "...             ...      ...      ...      ...            ...         ...  \n",
      "16125           0.0    98125  47.7091 -122.292           1250        7740  \n",
      "19004           0.0    98038  47.3505 -122.032           1780        6597  \n",
      "9094         1975.0    98116  47.5605 -122.402           1880        6135  \n",
      "3537            0.0    98146  47.4869 -122.340           1780        9520  \n",
      "10054           0.0    98052  47.6803 -122.156           1360        3088  \n",
      "\n",
      "[13397 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "houses = pd.read_csv('houses.csv', sep = ',',index_col=0)\n",
    "houses_new = pd.read_csv('houses_competition.csv', sep = ',',index_col=0)\n",
    "print (houses)\n",
    "# Train / Test split for houses:\n",
    "train , test = train_test_split(houses, test_size = 0.30,random_state=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL FUNCTIONS NEEDED FOR MODELLING, SELECTING VARIABLES AND PREDICTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function which creates regression model\n",
    "\"\"\"\n",
    "def my_regression(data,idx_p,idx_t):\n",
    "    X = data.iloc[:,idx_p]\n",
    "    X = sm.add_constant(X) \n",
    "    Y = data[data.columns.values.tolist()[idx_t]]\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "Function which makes prediction by any model on some data\n",
    "\"\"\"\n",
    "def my_prediction(my_model,data):\n",
    "    ranges=[]\n",
    "    for col in my_model.model.exog_names:\n",
    "        if(col!='const'):\n",
    "            ranges.append(data.columns.get_loc(col))\n",
    "    X_new = data.iloc[:,ranges]\n",
    "    X_new = sm.add_constant(X_new) # add the constant column\n",
    "    return my_model.predict(X_new)\n",
    "\n",
    "\"\"\"\n",
    "Function which computes generalization error\n",
    "\"\"\"\n",
    "def generalization_error_split(train,test,idx_p,idx_t):\n",
    "    model = my_regression(train,idx_p,idx_t)\n",
    "    prediction = my_prediction(model,test)\n",
    "    return np.mean((test[test.columns.values.tolist()[idx_t]] - prediction)**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORWARD SELECTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rsquared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strict stopping\n",
    "\"\"\"\n",
    "Function which implements first part of step selection for rsquared\n",
    "\"\"\"\n",
    "def step_selection_adj(train,v_s,v_nu,idx_t):\n",
    "    rsquareds=[] #To gather all the rsquareds adjusted.\n",
    "    for var in v_nu:\n",
    "        selects=[]\n",
    "        for local in v_s:\n",
    "            selects.append(local)\n",
    "        selects.append(var)\n",
    "        model = my_regression(train,selects,idx_t)\n",
    "        rsquareds.append(model.rsquared_adj)\n",
    "    index_max = np.argmax(rsquareds) #We find the highest rsquared_adj\n",
    "    return v_nu[index_max]\n",
    "\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for rsquared\n",
    "\"\"\"\n",
    "def foward_selection_adj(train,idx_p,idx_t):\n",
    "    v_s=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=-1\n",
    "    stop = False\n",
    "    while stop == False and len(v_nu)>0:\n",
    "        var = step_selection_adj(train,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        if my_regression(train,selects,idx_t).rsquared_adj>perf:\n",
    "            perf=my_regression(train,selects,idx_t).rsquared_adj\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "        else :\n",
    "            stop = True\n",
    "    v_s.sort()\n",
    "    return(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delta\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for rsquared\n",
    "\"\"\"\n",
    "def delta_foward_selection_adj(train,idx_p,idx_t):\n",
    "    v_s=[]\n",
    "    result = []\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=-1\n",
    "    stop = False\n",
    "    delta = 3 #Number of iterations\n",
    "    i=0\n",
    "    while (stop == False and len(v_nu)>0 and len(v_s)>0) or i<delta:\n",
    "        var = step_selection_adj(train,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        if my_regression(train,selects,idx_t).rsquared_adj>perf:\n",
    "            perf=my_regression(train,selects,idx_t).rsquared_adj\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "            i=0\n",
    "            result=v_s.copy()\n",
    "        else :\n",
    "            i+=1\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "            if(i==delta):\n",
    "                stop=True\n",
    "    result.sort()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for rsquared\n",
    "We gather all the best models by rsquaredadj and find the one with the best rsquared adj\n",
    "\"\"\"\n",
    "def c_foward_selection_adj(train,idx_p,idx_t):\n",
    "    v_s=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    models=[]\n",
    "    models_dict={}\n",
    "    while len(v_nu)>0:\n",
    "        var = step_selection_adj(train,v_s,v_nu,idx_t)\n",
    "        v_s.append(var)\n",
    "        v_nu.remove(var)\n",
    "        model = v_s.copy()\n",
    "        models.append(model)\n",
    "        models_dict[my_regression(train,model,idx_t).rsquared_adj]=model\n",
    "    rsquareds=[]\n",
    "    for model in models:\n",
    "        rsquareds.append(my_regression(train,model,idx_t).rsquared_adj)\n",
    "    print(models_dict)\n",
    "    index_max = np.argmax(rsquareds)\n",
    "    result=models[index_max]\n",
    "    result.sort()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strict stopping\n",
    "\"\"\"\n",
    "Function which implements first part of step selection for generalization error\n",
    "\"\"\"\n",
    "def step_selection_gen(train, valid, v_s,v_nu,idx_t):\n",
    "    gens=[]\n",
    "    for var in v_nu:\n",
    "        selects=[]\n",
    "        for local in v_s:\n",
    "            selects.append(local)\n",
    "        selects.append(var)\n",
    "        gens.append(generalization_error_split(train,valid,selects,idx_t))\n",
    "    index_min = np.argmin(gens)\n",
    "    return v_nu[index_min]\n",
    "\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "\"\"\"\n",
    "def foward_selection_gen(train,idx_p,idx_t):\n",
    "    train1 , valid = train_test_split(train, test_size = 0.30,random_state=5) \n",
    "    v_s=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=np.inf\n",
    "    stop = False\n",
    "    while stop == False and len(v_nu)>0:\n",
    "        var = step_selection_gen(train1,valid,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        if generalization_error_split(train1,valid,selects,idx_t) < perf:\n",
    "            perf=generalization_error_split(train1,valid,selects,idx_t)\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "        else :\n",
    "            stop = True\n",
    "    v_s.sort()\n",
    "    return(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delta\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "\"\"\"\n",
    "def delta_foward_selection_gen(train,idx_p,idx_t):\n",
    "    train1 , valid = train_test_split(train, test_size = 0.30,random_state=5) \n",
    "    v_s=[]\n",
    "    result = []\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=np.inf\n",
    "    stop = False\n",
    "    delta = 3\n",
    "    i=0\n",
    "    while (stop == False and len(v_nu)>0 and len(v_s)>0) or i<delta:\n",
    "        var = step_selection_gen(train1,valid,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        if generalization_error_split(train1,valid,selects,idx_t) < perf:\n",
    "            perf=generalization_error_split(train1,valid,selects,idx_t)\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "            i=0\n",
    "            result=v_s.copy()\n",
    "        else :\n",
    "            i+=1\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "            if(i==delta):\n",
    "                stop=True\n",
    "    result.sort()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "\"\"\"\n",
    "def c_foward_selection_gen(train,idx_p,idx_t):\n",
    "    train1 , valid = train_test_split(train, test_size = 0.30,random_state=5) \n",
    "    v_s=[]\n",
    "    models=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    models_dict={}\n",
    "    while len(v_nu)>0:\n",
    "        var = step_selection_gen(train1,valid,v_s,v_nu,idx_t)\n",
    "        v_s.append(var)\n",
    "        v_nu.remove(var)\n",
    "        model=v_s.copy()\n",
    "        models.append(model)\n",
    "        models_dict[generalization_error_split(train1,valid,model,idx_t)]=model\n",
    "    gens=[]\n",
    "    for model in models:\n",
    "            gens.append(generalization_error_split(train1,valid,model,idx_t))\n",
    "    index_min = np.argmin(gens)\n",
    "    print(models_dict)\n",
    "    result = models[index_min]\n",
    "    result.sort()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strict stopping\n",
    "\"\"\"\n",
    "Function which implements first part of step selection for generalization error\n",
    "We find the best variable by lowest pvalue. We cannot filter by pvalue<=0.05 yet.\n",
    "\"\"\"\n",
    "def step_selection_p(train,v_s,v_nu,idx_t):\n",
    "    pvalues=[]\n",
    "    for var in v_nu:\n",
    "        selects=[]\n",
    "        for local in v_s:\n",
    "            selects.append(local)\n",
    "        selects.append(var)\n",
    "        model = my_regression(train,selects,idx_t)\n",
    "        pvalues.append(model.pvalues[1])\n",
    "    index_min = np.argmin(pvalues)\n",
    "    print(v_nu[index_min],pvalues[index_min])\n",
    "    return v_nu[index_min]\n",
    "\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "We can filter by pvalue<=0.05. In order to find the best, we stop where the next one is significant\n",
    "\"\"\"\n",
    "def foward_selection_p(train,idx_p,idx_t):\n",
    "    v_s=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=False\n",
    "    stop = False\n",
    "    good = False\n",
    "    while stop == False and len(v_nu)>0:\n",
    "        var = step_selection_p(train,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        model = my_regression(train,selects,idx_t)\n",
    "        perf = model.pvalues[[i for i in range(1,len(selects))]]<=0.05\n",
    "        print(perf)\n",
    "        if False in perf.values:\n",
    "            good=False\n",
    "        else:\n",
    "            good=True\n",
    "        print(good)\n",
    "        if(good==True):\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "        else:\n",
    "            stop=True\n",
    "    v_s.sort()\n",
    "    return(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delta\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "\"\"\"\n",
    "def delta_foward_selection_p(train,idx_p,idx_t):\n",
    "    train1 , valid = train_test_split(train, test_size = 0.30) \n",
    "    v_s=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    perf=np.inf\n",
    "    stop = False\n",
    "    while stop == False and len(v_nu)>0:\n",
    "        var = step_selection_p(train1,valid,v_s,v_nu,idx_t)\n",
    "        selects = v_s.copy()\n",
    "        selects.append(var)\n",
    "        if generalization_error_split(train1,valid,selects,idx_t) < perf:\n",
    "            perf=generalization_error_split(train1,valid,selects,idx_t)\n",
    "            v_s.append(var)\n",
    "            v_nu.remove(var)\n",
    "        else :\n",
    "            stop = True\n",
    "    v_s.sort()\n",
    "    return(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous\n",
    "\"\"\"\n",
    "Function which implements second part of step selection for generalization error\n",
    "In order to find the best, we gather all the models. \n",
    "First, we filter by choosing models who don't contain no significant variable.\n",
    "Second, we choose the best model by generalisation error. To do so, we use the test split from the real houses data (named train).\n",
    "\"\"\"\n",
    "def c_foward_selection_p(train,test,idx_p,idx_t):\n",
    "    v_s=[]\n",
    "    models=[]\n",
    "    v_nu=idx_p.copy()\n",
    "    while len(v_nu)>0:\n",
    "        var = step_selection_p(train,v_s,v_nu,idx_t)\n",
    "        v_s.append(var)\n",
    "        v_nu.remove(var)\n",
    "        model=v_s.copy()\n",
    "        models.append(model)\n",
    "    print(models)\n",
    "    gens=[]\n",
    "    perfs=[]\n",
    "    finals=[]\n",
    "    for model in models:\n",
    "        perf=my_regression(train,model,idx_t).pvalues[[i for i in range(1,len(model))]]<=0.05\n",
    "        perfs.append(perf)\n",
    "    for model in models:\n",
    "        if (False not in perfs[models.index(model)].values):\n",
    "            finals.append(model)\n",
    "    for model in finals:\n",
    "        gens.append(generalization_error_split(train,test,model,idx_t))\n",
    "    index_min = np.argmin(gens)\n",
    "    result = models[index_min]\n",
    "    result.sort()\n",
    "    print(my_regression(train,result,idx_t).pvalues[[i for i in range(1,len(result))]]<=0.05)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function which adds polynomial features to variables that explain target variable\n",
    "\"\"\"\n",
    "def add_polynomial_feature(data, idx_p, power):\n",
    "    new_data = data.copy(deep = True)\n",
    "    for i in range(0, len(idx_p)):\n",
    "        for j in power:\n",
    "            for k in range(2, j+1):\n",
    "                new_data['{}_pow_{}'.format(new_data.columns[idx_p[i]],k)] = new_data.iloc[:,idx_p[i]]**k\n",
    "    return(new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FOR NON-LINEAR REGRESSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- SIMPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms\n",
      "bathrooms\n",
      "sqft_living\n",
      "sqft_lot\n",
      "floors\n",
      "waterfront\n",
      "view\n",
      "condition\n",
      "grade\n",
      "sqft_above\n",
      "sqft_basement\n",
      "yr_built\n",
      "yr_renovated\n",
      "zipcode\n",
      "lat\n",
      "long\n",
      "sqft_living15\n",
      "sqft_lot15\n",
      "[11.97152747664599, 9.1912892158212, 6.528861183289393, 12.872168109274229, 11.786838650789518, 12.043192123875176, 10.88388716570048, 12.932567474395945, 7.043386826290524, 7.859229190321266, 11.816525316930369, 12.894049063948652, 12.824287846405896, 12.908346253404964, 11.572645215546652, 12.92671145535201, 8.301113957182567, 12.85473468729375]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.320e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 02 May 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:08:34</td>     <th>  Log-Likelihood:    </th> <td> -32042.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13397</td>      <th>  AIC:               </th> <td>6.409e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13395</td>      <th>  BIC:               </th> <td>6.410e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>   -0.5466</td> <td>    0.057</td> <td>   -9.646</td> <td> 0.000</td> <td>   -0.658</td> <td>   -0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th> <td>    0.0029</td> <td> 2.49e-05</td> <td>  114.907</td> <td> 0.000</td> <td>    0.003</td> <td>    0.003</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>9718.781</td> <th>  Durbin-Watson:     </th>  <td>   1.981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>441688.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.993</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>30.485</td>  <th>  Cond. No.          </th>  <td>5.65e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.65e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.496\n",
       "Model:                            OLS   Adj. R-squared:                  0.496\n",
       "Method:                 Least Squares   F-statistic:                 1.320e+04\n",
       "Date:                Tue, 02 May 2023   Prob (F-statistic):               0.00\n",
       "Time:                        01:08:34   Log-Likelihood:                -32042.\n",
       "No. Observations:               13397   AIC:                         6.409e+04\n",
       "Df Residuals:                   13395   BIC:                         6.410e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const          -0.5466      0.057     -9.646      0.000      -0.658      -0.436\n",
       "sqft_living     0.0029   2.49e-05    114.907      0.000       0.003       0.003\n",
       "==============================================================================\n",
       "Omnibus:                     9718.781   Durbin-Watson:                   1.981\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           441688.826\n",
       "Skew:                           2.993   Prob(JB):                         0.00\n",
       "Kurtosis:                      30.485   Cond. No.                     5.65e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.65e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens=[]\n",
    "for i in range(1,len(test.columns)):\n",
    "    mse = generalization_error_split(train,test,i,0)\n",
    "    gens.append(mse)\n",
    "    print(test.columns[i])\n",
    "print(gens)\n",
    "index_mint = np.argmin(gens)+1 #Because we have price which is 1\n",
    "modelsimple = my_regression(houses,index_mint,0)\n",
    "modelsimple.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.828042477695651\n"
     ]
    }
   ],
   "source": [
    "modelmultiple = my_regression(houses,[i for i in range(1,len(test.columns))],0)\n",
    "modelmultiple.summary()\n",
    "print(generalization_error_split(train,test,[i for i in range(1,len(test.columns))],0))\n",
    "value = modelmultiple.pvalues[[i for i in range(1,len(test.columns))]]<=0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- VARIABLE SELECTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- FORWARD SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "3.828790195822256\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "3.828790195822256\n",
      "{0.4965996647057377: [3], 0.5673234852878521: [3, 15], 0.6052345382611868: [3, 15, 6], 0.6297135358020418: [3, 15, 6, 9], 0.6695178638312591: [3, 15, 6, 9, 12], 0.676184455344699: [3, 15, 6, 9, 12, 7], 0.6806878443804338: [3, 15, 6, 9, 12, 7, 1], 0.6852531888584019: [3, 15, 6, 9, 12, 7, 1, 2], 0.6880300297515674: [3, 15, 6, 9, 12, 7, 1, 2, 14], 0.6913914900047977: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16], 0.6925707406143718: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11], 0.6937087396105793: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8], 0.6941105325700836: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13], 0.694325143286076: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13, 18], 0.6944571087985782: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13, 18, 17], 0.6944560977732497: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13, 18, 17, 10], 0.6944258155351066: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13, 18, 17, 10, 4], 0.6943940278546918: [3, 15, 6, 9, 12, 7, 1, 2, 14, 16, 11, 8, 13, 18, 17, 10, 4, 5]}\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "3.828790195822256\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15529     3.890133\n",
       "3233      3.334325\n",
       "14381     6.162741\n",
       "3201     12.231634\n",
       "3425      1.982928\n",
       "           ...    \n",
       "7989      4.949858\n",
       "9973      3.586979\n",
       "9007      4.250436\n",
       "16345    13.749974\n",
       "13582     0.375648\n",
       "Length: 2365, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-1- BASED ON RSQUARED ADJUSTED\n",
    "\n",
    "# 1-1-a- Strict stopping criterion\n",
    "selected_by_rsquared = foward_selection_adj(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_rsquared)\n",
    "print(generalization_error_split(train,test,selected_by_rsquared,0))\n",
    "\n",
    "# 1-1-b- Delta iteration criterion\n",
    "selected_by_delta_rsquared = delta_foward_selection_adj(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_delta_rsquared)\n",
    "print(generalization_error_split(train,test,selected_by_delta_rsquared,0))\n",
    "\n",
    "# 1-1-c- Continuous criterion\n",
    "selected_by_continuous_rsquared = c_foward_selection_adj(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_continuous_rsquared)\n",
    "print(generalization_error_split(train,test,selected_by_continuous_rsquared,0))\n",
    "\n",
    "gens=[generalization_error_split(train,test,selected_by_rsquared,0),generalization_error_split(train,test,selected_by_delta_rsquared,0),generalization_error_split(train,test,selected_by_continuous_rsquared,0)]\n",
    "vars = [selected_by_rsquared,selected_by_delta_rsquared,selected_by_continuous_rsquared]\n",
    "index_min = np.argmin(gens)\n",
    "selectedbyrsquared = vars[index_min]\n",
    "print(selectedbyrsquared)\n",
    "\n",
    "modelrsquaredforward = my_regression(houses,selectedbyrsquared,0)\n",
    "modelrsquaredforward.summary()\n",
    "my_prediction(modelrsquaredforward,houses_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17]\n",
      "3.8306615031281295\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17]\n",
      "3.8306615031281295\n",
      "{5.776177880221217: [3], 4.767671875821275: [3, 15], 4.35828884973035: [3, 15, 7], 4.000935108871944: [3, 15, 7, 9], 3.660705109751807: [3, 15, 7, 9, 12], 3.5270855470950715: [3, 15, 7, 9, 12, 6], 3.490067751099924: [3, 15, 7, 9, 12, 6, 2], 3.4487686701085605: [3, 15, 7, 9, 12, 6, 2, 1], 3.41290930783081: [3, 15, 7, 9, 12, 6, 2, 1, 14], 3.3983797357833683: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11], 3.373882421644576: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16], 3.3571030975198566: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8], 3.347490348266162: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13], 3.34409096049492: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13, 17], 3.345963069556161: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13, 17, 10], 3.349688344950397: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13, 17, 10, 5], 3.35696670875732: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13, 17, 10, 5, 18], 3.362338971522398: [3, 15, 7, 9, 12, 6, 2, 1, 14, 11, 16, 8, 13, 17, 10, 5, 18, 4]}\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17]\n",
      "3.8306615031281295\n",
      "[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.698</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.698</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2209.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 02 May 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:08:44</td>     <th>  Log-Likelihood:    </th> <td> -28618.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13397</td>      <th>  AIC:               </th> <td>5.727e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13382</td>      <th>  BIC:               </th> <td>5.738e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td>   26.3896</td> <td>   37.204</td> <td>    0.709</td> <td> 0.478</td> <td>  -46.535</td> <td>   99.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>      <td>   -0.3631</td> <td>    0.024</td> <td>  -15.116</td> <td> 0.000</td> <td>   -0.410</td> <td>   -0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td>    0.4775</td> <td>    0.041</td> <td>   11.697</td> <td> 0.000</td> <td>    0.397</td> <td>    0.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th>   <td>    0.0019</td> <td> 4.62e-05</td> <td>   41.588</td> <td> 0.000</td> <td>    0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td>    6.4389</td> <td>    0.235</td> <td>   27.457</td> <td> 0.000</td> <td>    5.979</td> <td>    6.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>          <td>    0.5118</td> <td>    0.028</td> <td>   18.340</td> <td> 0.000</td> <td>    0.457</td> <td>    0.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>     <td>    0.2469</td> <td>    0.030</td> <td>    8.125</td> <td> 0.000</td> <td>    0.187</td> <td>    0.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>         <td>    0.9345</td> <td>    0.028</td> <td>   33.606</td> <td> 0.000</td> <td>    0.880</td> <td>    0.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_basement</th> <td>   -0.0004</td> <td> 4.97e-05</td> <td>   -8.294</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_built</th>      <td>   -0.0271</td> <td>    0.001</td> <td>  -29.635</td> <td> 0.000</td> <td>   -0.029</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>  <td>    0.0002</td> <td> 4.74e-05</td> <td>    3.753</td> <td> 0.000</td> <td>  8.5e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zipcode</th>       <td>   -0.0056</td> <td>    0.000</td> <td>  -13.182</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat</th>           <td>    6.0742</td> <td>    0.138</td> <td>   44.108</td> <td> 0.000</td> <td>    5.804</td> <td>    6.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long</th>          <td>   -2.3082</td> <td>    0.166</td> <td>  -13.878</td> <td> 0.000</td> <td>   -2.634</td> <td>   -1.982</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td>    0.0002</td> <td> 4.47e-05</td> <td>    3.533</td> <td> 0.000</td> <td> 7.03e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>11934.920</td> <th>  Durbin-Watson:     </th>  <td>   1.994</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>1424452.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.792</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>52.943</td>   <th>  Cond. No.          </th>  <td>2.06e+08</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.06e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.698\n",
       "Method:                 Least Squares   F-statistic:                     2209.\n",
       "Date:                Tue, 02 May 2023   Prob (F-statistic):               0.00\n",
       "Time:                        01:08:44   Log-Likelihood:                -28618.\n",
       "No. Observations:               13397   AIC:                         5.727e+04\n",
       "Df Residuals:                   13382   BIC:                         5.738e+04\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const            26.3896     37.204      0.709      0.478     -46.535      99.314\n",
       "bedrooms         -0.3631      0.024    -15.116      0.000      -0.410      -0.316\n",
       "bathrooms         0.4775      0.041     11.697      0.000       0.397       0.558\n",
       "sqft_living       0.0019   4.62e-05     41.588      0.000       0.002       0.002\n",
       "waterfront        6.4389      0.235     27.457      0.000       5.979       6.899\n",
       "view              0.5118      0.028     18.340      0.000       0.457       0.566\n",
       "condition         0.2469      0.030      8.125      0.000       0.187       0.306\n",
       "grade             0.9345      0.028     33.606      0.000       0.880       0.989\n",
       "sqft_basement    -0.0004   4.97e-05     -8.294      0.000      -0.001      -0.000\n",
       "yr_built         -0.0271      0.001    -29.635      0.000      -0.029      -0.025\n",
       "yr_renovated      0.0002   4.74e-05      3.753      0.000     8.5e-05       0.000\n",
       "zipcode          -0.0056      0.000    -13.182      0.000      -0.006      -0.005\n",
       "lat               6.0742      0.138     44.108      0.000       5.804       6.344\n",
       "long             -2.3082      0.166    -13.878      0.000      -2.634      -1.982\n",
       "sqft_living15     0.0002   4.47e-05      3.533      0.000    7.03e-05       0.000\n",
       "==============================================================================\n",
       "Omnibus:                    11934.920   Durbin-Watson:                   1.994\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1424452.366\n",
       "Skew:                           3.792   Prob(JB):                         0.00\n",
       "Kurtosis:                      52.943   Cond. No.                     2.06e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.06e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-2- BASED ON GENERALISATION ERROR\n",
    "\n",
    "# 1-1-a- Strict stopping criterion\n",
    "selected_by_gen = foward_selection_gen(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_gen)\n",
    "print(generalization_error_split(train,test,selected_by_gen,0))\n",
    "\n",
    "# 1-1-b- Delta iteration criterion\n",
    "selected_by_gen_delta = delta_foward_selection_gen(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_gen_delta)\n",
    "print(generalization_error_split(train,test,selected_by_gen_delta,0))\n",
    "\n",
    "# 1-1-c- Continuous criterion\n",
    "selected_by_continuous_gen = c_foward_selection_gen(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_continuous_gen)\n",
    "print(generalization_error_split(train,test,selected_by_continuous_gen,0))\n",
    "\n",
    "gens=[generalization_error_split(train,test,selected_by_gen,0),generalization_error_split(train,test,selected_by_gen_delta,0),generalization_error_split(train,test,selected_by_continuous_gen,0)]\n",
    "vars = [selected_by_gen,selected_by_gen_delta,selected_by_continuous_gen]\n",
    "index_min = np.argmin(gens)\n",
    "selectedbygen=vars[index_min]\n",
    "print(selectedbygen)\n",
    "\n",
    "modelgenforward=my_regression(houses,selectedbygen,0)\n",
    "modelgenforward.summary()\n",
    "#my_prediction(modelgenforward,houses_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.0\n",
      "Series([], dtype: bool)\n",
      "True\n",
      "1 0.0\n",
      "bathrooms    True\n",
      "dtype: bool\n",
      "True\n",
      "4 0.0\n",
      "bathrooms    True\n",
      "bedrooms     True\n",
      "dtype: bool\n",
      "True\n",
      "5 0.0\n",
      "bathrooms    True\n",
      "bedrooms     True\n",
      "sqft_lot     True\n",
      "dtype: bool\n",
      "True\n",
      "6 0.0\n",
      "bathrooms     True\n",
      "bedrooms      True\n",
      "sqft_lot      True\n",
      "floors       False\n",
      "dtype: bool\n",
      "False\n",
      "[1, 2, 4, 5]\n",
      "9.176074327093731\n",
      "2 0.0\n",
      "1 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "8 0.0\n",
      "12 0.0\n",
      "7 0.0\n",
      "11 0.0\n",
      "13 0.0\n",
      "14 0.0\n",
      "16 0.0\n",
      "15 0.0\n",
      "18 0.0\n",
      "17 2.7780052005869945e-178\n",
      "9 2.837186756072864e-100\n",
      "10 2.3182511500740393e-30\n",
      "3 2.0016906199978574e-25\n",
      "[[2], [2, 1], [2, 1, 4], [2, 1, 4, 5], [2, 1, 4, 5, 6], [2, 1, 4, 5, 6, 8], [2, 1, 4, 5, 6, 8, 12], [2, 1, 4, 5, 6, 8, 12, 7], [2, 1, 4, 5, 6, 8, 12, 7, 11], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15, 18], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15, 18, 17], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15, 18, 17, 9], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15, 18, 17, 9, 10], [2, 1, 4, 5, 6, 8, 12, 7, 11, 13, 14, 16, 15, 18, 17, 9, 10, 3]]\n",
      "bedrooms      True\n",
      "bathrooms     True\n",
      "sqft_lot      True\n",
      "floors        True\n",
      "waterfront    True\n",
      "view          True\n",
      "condition     True\n",
      "dtype: bool\n",
      "[1, 2, 4, 5, 6, 7, 8, 12]\n",
      "7.245061856607893\n",
      "[1, 2, 4, 5, 6, 7, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "# 1-3- BASED ON CRITICAL PROBABILITY\n",
    "\n",
    "# 1-1-a- Strict stopping criterion\n",
    "selected_by_pvalue=foward_selection_p(train,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_pvalue)\n",
    "print(generalization_error_split(train,test,selected_by_pvalue,0))\n",
    "#modelStrictCritcProb = my_regression(houses,foward_selection_p(train,[i for i in range(2,len(train.columns))],1),1)\n",
    "\n",
    "\n",
    "# 1-1-b- Delta iteration criterion\n",
    "\n",
    "# 1-1-c- Continuous criterion\n",
    "selected_by_pvalues_continue = c_foward_selection_p(train,test,[i for i in range(1,len(train.columns))],0)\n",
    "print(selected_by_pvalues_continue)\n",
    "print(generalization_error_split(train,test,selected_by_pvalues_continue,0))\n",
    "\n",
    "gens=[generalization_error_split(train,test,selected_by_pvalue,0),generalization_error_split(train,test,selected_by_pvalues_continue,0),]\n",
    "vars = [selected_by_pvalue,selected_by_pvalues_continue]\n",
    "index_min = np.argmin(gens)\n",
    "selectedbypvalues=vars[index_min]\n",
    "print(selectedbypvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- NON-LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
      "10928  4.4800         2       1.50         1630      3780     1.0         0.0   \n",
      "1171   3.5900         3       2.50         1450      3850     2.0         0.0   \n",
      "6401   4.7000         3       1.00         1460      8227     1.0         0.0   \n",
      "12572  6.4800         5       2.25         2410     12000     2.0         0.0   \n",
      "2499   5.6000         4       1.75         2150      8555     1.0         0.0   \n",
      "...       ...       ...        ...          ...       ...     ...         ...   \n",
      "13633  4.1850         3       2.00         1800     12440     1.0         0.0   \n",
      "11496  5.1000         3       2.25         2750    219542     2.0         0.0   \n",
      "10414  3.8900         3       1.50         2030     10075     1.0         0.0   \n",
      "871    4.6995         3       2.25         1620      8701     1.0         0.0   \n",
      "7738   2.7500         4       1.00         1770      7345     1.5         0.0   \n",
      "\n",
      "       view  condition  grade  ...     lat_pow_4    long_pow_2    long_pow_3  \\\n",
      "10928   0.0          4      7  ...  5.164410e+06  14982.494409 -1.833902e+06   \n",
      "1171    0.0          3      7  ...  5.104310e+06  14847.422500 -1.809158e+06   \n",
      "6401    0.0          3      6  ...  5.177335e+06  14957.534601 -1.829321e+06   \n",
      "12572   0.0          4      8  ...  5.109295e+06  14926.730625 -1.823673e+06   \n",
      "2499    2.0          4      7  ...  5.105427e+06  14968.299025 -1.831297e+06   \n",
      "...     ...        ...    ...  ...           ...           ...           ...   \n",
      "13633   0.0          3      8  ...  5.144635e+06  14896.202500 -1.818082e+06   \n",
      "11496   0.0          3      7  ...  5.149519e+06  14871.314704 -1.813527e+06   \n",
      "10414   0.0          5      7  ...  5.078372e+06  14932.106809 -1.824659e+06   \n",
      "871     0.0          3      7  ...  5.136732e+06  14917.446769 -1.821972e+06   \n",
      "7738    0.0          3      7  ...  5.107532e+06  14969.767201 -1.831566e+06   \n",
      "\n",
      "         long_pow_4  sqft_living15_pow_2  sqft_living15_pow_3  \\\n",
      "10928  2.244751e+08              3132900           5545233000   \n",
      "1171   2.204460e+08              3880900           7645373000   \n",
      "6401   2.237278e+08              2340900           3581577000   \n",
      "12572  2.228073e+08              4326400           8998912000   \n",
      "2499   2.240500e+08              2190400           3241792000   \n",
      "...             ...                  ...                  ...   \n",
      "13633  2.218968e+08              6051600          14886936000   \n",
      "11496  2.211560e+08              5904900          14348907000   \n",
      "10414  2.229678e+08              4884100          10793861000   \n",
      "871    2.225302e+08              2560000           4096000000   \n",
      "7738   2.240939e+08              2496400           3944312000   \n",
      "\n",
      "       sqft_living15_pow_4  sqft_lot15_pow_2   sqft_lot15_pow_3  \\\n",
      "10928        9815062410000          40960000       262144000000   \n",
      "1171        15061384810000          14047504        52650044992   \n",
      "6401         5479812810000          66064384       536971313152   \n",
      "12572       18717736960000         144000000      1728000000000   \n",
      "2499         4797852160000          54834025       406045955125   \n",
      "...                    ...               ...                ...   \n",
      "13633       36621862560000         152571904      1884568158208   \n",
      "11496       34867844010000       48198689764  10581636748168088   \n",
      "10414       23854432810000         101505625      1022669171875   \n",
      "871          6553600000000          62568100       494913671000   \n",
      "7738         6232012960000          53949025       396255588625   \n",
      "\n",
      "          sqft_lot15_pow_4  \n",
      "10928     1677721600000000  \n",
      "1171       197332368630016  \n",
      "6401      4364502833299456  \n",
      "12572    20736000000000000  \n",
      "2499      3006770297700625  \n",
      "...                    ...  \n",
      "13633    23278185890185216  \n",
      "11496 -1176058321085127920  \n",
      "10414    10303391906640625  \n",
      "871       3914767137610000  \n",
      "7738      2910497298450625  \n",
      "\n",
      "[9377 rows x 73 columns]\n"
     ]
    }
   ],
   "source": [
    "train_poly = add_polynomial_feature(train,[i for i in range(1,len(train.columns))] ,[j for j in range(2,5)])\n",
    "test_poly = add_polynomial_feature(test, [i for i in range(1,len(test.columns))] ,[j for j in range(2,5)])\n",
    "print(train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.128364934326332: [25], 4.182052198434985: [25, 15], 3.6985566676584685: [25, 15, 45], 3.3612865368461224: [25, 15, 45, 12], 3.168652704013481: [25, 15, 45, 12, 37], 3.082112731402216: [25, 15, 45, 12, 37, 63], 2.765884901284438: [25, 15, 45, 12, 37, 63, 62], 2.6765921733951523: [25, 15, 45, 12, 37, 63, 62, 6], 2.6200897423492644: [25, 15, 45, 12, 37, 63, 62, 6, 23], 2.5796879320205153: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8], 2.541192237940801: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17], 2.502728205979586: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65], 2.4587626235339743: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14], 2.434545552541056: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13], 2.4208100569971456: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49], 2.4034279114256067: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46], 2.3946839122326997: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67], 2.378847001564399: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44], 2.364851556909471: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43], 2.353592835826112: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10], 2.34175834610102: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9], 2.333340671268783: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19], 2.327399709197052: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55], 2.32068696361167: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52], 2.3155858361786765: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5], 2.311492484871393: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7], 2.3090485893601946: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38], 2.30064826060916: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39], 2.2980155480317155: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4], 2.296584880700423: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64], 2.296345463625764: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61], 2.2962310397254293: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32], 2.2964024030969754: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34], 2.2962762687743767: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35], 2.2963477484978423: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36], 2.2969023234903148: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40], 2.2976917472321583: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41], 2.2973329544188306: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42], 2.2985680157503854: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21], 2.299594216518747: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33], 2.302303502217042: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2], 2.30503719444311: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18], 2.30723491289418: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31], 2.311241095257402: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1], 2.314767579366592: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20], 2.3201419102337737: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3], 2.3239907140722353: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11], 2.329691471994857: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22], 2.3264698010944276: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24], 2.335247220441819: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16], 2.3306203957983387: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56], 2.3567784014755517: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66], 2.340905864344707: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53], 2.3334377371110757: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58], 2.3384141855417786: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68], 2.333597869640599: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47], 2.3504700157862306: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28], 2.347431328800106: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70], 2.5677371844293417: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50], 2.7528537856566317: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26], 2.774431891676635: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54], 2.7741393772147163: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57], 2.7751529564635073: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27], 2.750232303451567: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59], 2.748354277339007: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48], 2.7573078465057517: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69], 2.8556928008485665: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71], 2.9157265150874356: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71, 29], 3.111891830646271: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71, 29, 51], 3.352977062105152: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71, 29, 51, 60], 3.502208829723171: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71, 29, 51, 60, 72], 3.601868305277662: [25, 15, 45, 12, 37, 63, 62, 6, 23, 8, 17, 65, 14, 13, 49, 46, 67, 44, 43, 10, 9, 19, 55, 52, 5, 7, 38, 39, 4, 64, 61, 32, 34, 35, 36, 40, 41, 42, 21, 33, 2, 18, 31, 1, 20, 3, 11, 22, 24, 16, 56, 66, 53, 58, 68, 47, 28, 70, 50, 26, 54, 57, 27, 59, 48, 69, 71, 29, 51, 60, 72, 30]}\n",
      "[4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 17, 19, 23, 25, 32, 37, 38, 39, 43, 44, 45, 46, 49, 52, 55, 61, 62, 63, 64, 65, 67]\n",
      "3.0695030715807126\n"
     ]
    }
   ],
   "source": [
    "# 1- POLYNOMIAL REGRESSION (By Generalisation error only)\n",
    "#By strict stopping criterion\n",
    "select_strict = foward_selection_gen(train_poly,[i for i in range(1,len(train_poly.columns))],0)\n",
    "\n",
    "#By delta iteration\n",
    "select_delta = delta_foward_selection_gen(train_poly,[i for i in range(1,len(train_poly.columns))],0)\n",
    "\n",
    "#By continuous criterion\n",
    "select_continuous = c_foward_selection_gen(train_poly,[i for i in range(1,len(train_poly.columns))],0)\n",
    "\n",
    "gens=[generalization_error_split(train_poly,test_poly,select_strict,0),generalization_error_split(train_poly,test_poly,select_delta,0),generalization_error_split(train_poly,test_poly,select_continuous,0)]\n",
    "vars = [select_strict,select_delta,select_continuous]\n",
    "index_min = np.argmin(gens)\n",
    "selectpolynomial=vars[index_min]\n",
    "print(selectpolynomial)\n",
    "print(generalization_error_split(train_poly,test_poly,selectpolynomial,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.781</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.780</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1534.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 02 May 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:11:02</td>     <th>  Log-Likelihood:    </th> <td> -26478.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13397</td>      <th>  AIC:               </th> <td>5.302e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13365</td>      <th>  BIC:               </th> <td>5.326e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>               <td> 3.901e+04</td> <td> 1012.136</td> <td>   38.546</td> <td> 0.000</td> <td>  3.7e+04</td> <td>  4.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>            <td> 9.751e-07</td> <td> 4.06e-07</td> <td>    2.401</td> <td> 0.016</td> <td> 1.79e-07</td> <td> 1.77e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>              <td>   -0.3226</td> <td>    0.100</td> <td>   -3.236</td> <td> 0.001</td> <td>   -0.518</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>          <td>    6.5298</td> <td>    0.229</td> <td>   28.516</td> <td> 0.000</td> <td>    6.081</td> <td>    6.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>                <td>    3.3408</td> <td>    0.555</td> <td>    6.017</td> <td> 0.000</td> <td>    2.252</td> <td>    4.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>           <td>    0.4116</td> <td>    0.026</td> <td>   15.676</td> <td> 0.000</td> <td>    0.360</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>               <td>  -13.0548</td> <td>    2.552</td> <td>   -5.116</td> <td> 0.000</td> <td>  -18.056</td> <td>   -8.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above</th>          <td>    0.0007</td> <td> 8.93e-05</td> <td>    8.012</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_built</th>            <td>   -0.5538</td> <td>    0.081</td> <td>   -6.877</td> <td> 0.000</td> <td>   -0.712</td> <td>   -0.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>        <td>   -0.0315</td> <td>    0.005</td> <td>   -6.422</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zipcode</th>             <td>   -0.0064</td> <td>    0.000</td> <td>  -16.462</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat</th>                 <td> 4.623e+05</td> <td>  1.2e+04</td> <td>   38.546</td> <td> 0.000</td> <td> 4.39e+05</td> <td> 4.86e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th>       <td>    0.0009</td> <td>    0.000</td> <td>    7.627</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms_pow_2</th>      <td>   -0.0024</td> <td>    0.001</td> <td>   -1.734</td> <td> 0.083</td> <td>   -0.005</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms_pow_3</th>     <td>    0.0144</td> <td>    0.001</td> <td>    9.837</td> <td> 0.000</td> <td>    0.012</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_pow_2</th>   <td> 3.815e-07</td> <td> 1.69e-08</td> <td>   22.539</td> <td> 0.000</td> <td> 3.48e-07</td> <td> 4.15e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_pow_3</th>        <td>    0.0070</td> <td>    0.009</td> <td>    0.762</td> <td> 0.446</td> <td>   -0.011</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_pow_2</th>          <td>   -3.4018</td> <td>    0.680</td> <td>   -5.006</td> <td> 0.000</td> <td>   -4.734</td> <td>   -2.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_pow_3</th>          <td>    1.2265</td> <td>    0.266</td> <td>    4.618</td> <td> 0.000</td> <td>    0.706</td> <td>    1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_pow_4</th>          <td>   -0.1386</td> <td>    0.033</td> <td>   -4.220</td> <td> 0.000</td> <td>   -0.203</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_pow_2</th>         <td>    2.7156</td> <td>    0.479</td> <td>    5.669</td> <td> 0.000</td> <td>    1.777</td> <td>    3.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_pow_3</th>         <td>   -0.2454</td> <td>    0.039</td> <td>   -6.264</td> <td> 0.000</td> <td>   -0.322</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_pow_4</th>         <td>    0.0085</td> <td>    0.001</td> <td>    7.189</td> <td> 0.000</td> <td>    0.006</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above_pow_2</th>    <td> -2.62e-07</td> <td> 2.48e-08</td> <td>  -10.552</td> <td> 0.000</td> <td>-3.11e-07</td> <td>-2.13e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_basement_pow_2</th> <td>-7.747e-07</td> <td> 6.78e-08</td> <td>  -11.426</td> <td> 0.000</td> <td>-9.08e-07</td> <td>-6.42e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_built_pow_2</th>      <td>    0.0001</td> <td> 2.06e-05</td> <td>    6.760</td> <td> 0.000</td> <td> 9.86e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated_pow_2</th>  <td> 1.597e-05</td> <td> 2.46e-06</td> <td>    6.504</td> <td> 0.000</td> <td> 1.12e-05</td> <td> 2.08e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat_pow_2</th>           <td> -2.93e+04</td> <td>  760.242</td> <td>  -38.540</td> <td> 0.000</td> <td>-3.08e+04</td> <td>-2.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat_pow_3</th>           <td>  617.6954</td> <td>   16.022</td> <td>   38.554</td> <td> 0.000</td> <td>  586.291</td> <td>  649.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat_pow_4</th>           <td>   -4.3386</td> <td>    0.112</td> <td>  -38.573</td> <td> 0.000</td> <td>   -4.559</td> <td>   -4.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long_pow_2</th>          <td>   -1.0723</td> <td>    0.559</td> <td>   -1.918</td> <td> 0.055</td> <td>   -2.168</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long_pow_3</th>          <td>   -0.0059</td> <td>    0.003</td> <td>   -1.941</td> <td> 0.052</td> <td>   -0.012</td> <td>  5.8e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15_pow_2</th> <td>-9.803e-08</td> <td>  2.5e-08</td> <td>   -3.921</td> <td> 0.000</td> <td>-1.47e-07</td> <td> -4.9e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>6700.045</td> <th>  Durbin-Watson:     </th>  <td>   1.980</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>875400.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.396</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>42.502</td>  <th>  Cond. No.          </th>  <td>2.68e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.62e-15. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.781\n",
       "Model:                            OLS   Adj. R-squared:                  0.780\n",
       "Method:                 Least Squares   F-statistic:                     1534.\n",
       "Date:                Tue, 02 May 2023   Prob (F-statistic):               0.00\n",
       "Time:                        01:11:02   Log-Likelihood:                -26478.\n",
       "No. Observations:               13397   AIC:                         5.302e+04\n",
       "Df Residuals:                   13365   BIC:                         5.326e+04\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=======================================================================================\n",
       "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------\n",
       "const                3.901e+04   1012.136     38.546      0.000     3.7e+04     4.1e+04\n",
       "sqft_lot             9.751e-07   4.06e-07      2.401      0.016    1.79e-07    1.77e-06\n",
       "floors                 -0.3226      0.100     -3.236      0.001      -0.518      -0.127\n",
       "waterfront              6.5298      0.229     28.516      0.000       6.081       6.979\n",
       "view                    3.3408      0.555      6.017      0.000       2.252       4.429\n",
       "condition               0.4116      0.026     15.676      0.000       0.360       0.463\n",
       "grade                 -13.0548      2.552     -5.116      0.000     -18.056      -8.053\n",
       "sqft_above              0.0007   8.93e-05      8.012      0.000       0.001       0.001\n",
       "yr_built               -0.5538      0.081     -6.877      0.000      -0.712      -0.396\n",
       "yr_renovated           -0.0315      0.005     -6.422      0.000      -0.041      -0.022\n",
       "zipcode                -0.0064      0.000    -16.462      0.000      -0.007      -0.006\n",
       "lat                  4.623e+05    1.2e+04     38.546      0.000    4.39e+05    4.86e+05\n",
       "sqft_living15           0.0009      0.000      7.627      0.000       0.001       0.001\n",
       "bedrooms_pow_2         -0.0024      0.001     -1.734      0.083      -0.005       0.000\n",
       "bathrooms_pow_3         0.0144      0.001      9.837      0.000       0.012       0.017\n",
       "sqft_living_pow_2    3.815e-07   1.69e-08     22.539      0.000    3.48e-07    4.15e-07\n",
       "floors_pow_3            0.0070      0.009      0.762      0.446      -0.011       0.025\n",
       "view_pow_2             -3.4018      0.680     -5.006      0.000      -4.734      -2.070\n",
       "view_pow_3              1.2265      0.266      4.618      0.000       0.706       1.747\n",
       "view_pow_4             -0.1386      0.033     -4.220      0.000      -0.203      -0.074\n",
       "grade_pow_2             2.7156      0.479      5.669      0.000       1.777       3.655\n",
       "grade_pow_3            -0.2454      0.039     -6.264      0.000      -0.322      -0.169\n",
       "grade_pow_4             0.0085      0.001      7.189      0.000       0.006       0.011\n",
       "sqft_above_pow_2     -2.62e-07   2.48e-08    -10.552      0.000   -3.11e-07   -2.13e-07\n",
       "sqft_basement_pow_2 -7.747e-07   6.78e-08    -11.426      0.000   -9.08e-07   -6.42e-07\n",
       "yr_built_pow_2          0.0001   2.06e-05      6.760      0.000    9.86e-05       0.000\n",
       "yr_renovated_pow_2   1.597e-05   2.46e-06      6.504      0.000    1.12e-05    2.08e-05\n",
       "lat_pow_2            -2.93e+04    760.242    -38.540      0.000   -3.08e+04   -2.78e+04\n",
       "lat_pow_3             617.6954     16.022     38.554      0.000     586.291     649.100\n",
       "lat_pow_4              -4.3386      0.112    -38.573      0.000      -4.559      -4.118\n",
       "long_pow_2             -1.0723      0.559     -1.918      0.055      -2.168       0.023\n",
       "long_pow_3             -0.0059      0.003     -1.941      0.052      -0.012     5.8e-05\n",
       "sqft_living15_pow_2 -9.803e-08    2.5e-08     -3.921      0.000   -1.47e-07    -4.9e-08\n",
       "==============================================================================\n",
       "Omnibus:                     6700.045   Durbin-Watson:                   1.980\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           875400.213\n",
       "Skew:                           1.396   Prob(JB):                         0.00\n",
       "Kurtosis:                      42.502   Cond. No.                     2.68e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.62e-15. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_houses=add_polynomial_feature(houses,[i for i in range(1,len(houses.columns))] ,[j for j in range(2,5)])\n",
    "poly_houses_new=add_polynomial_feature(houses_new,[i for i in range(0,len(houses_new.columns))] ,[j for j in range(2,5)])\n",
    "modelpolynomial = my_regression(poly_houses,selectpolynomial,0)\n",
    "modelpolynomial.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V- SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15529     5.568317\n",
      "3233      2.996609\n",
      "14381     4.396761\n",
      "3201     13.406311\n",
      "3425      4.911103\n",
      "           ...    \n",
      "7989      6.111233\n",
      "9973      4.796804\n",
      "9007      3.853845\n",
      "16345    10.911754\n",
      "13582     2.196522\n",
      "Length: 2365, dtype: float64\n",
      "15529     3.878332\n",
      "3233      3.338104\n",
      "14381     6.191181\n",
      "3201     12.233500\n",
      "3425      1.996580\n",
      "           ...    \n",
      "7989      4.961400\n",
      "9973      3.593966\n",
      "9007      4.232455\n",
      "16345    13.751428\n",
      "13582     0.376524\n",
      "Length: 2365, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "predictionsimple = my_prediction(modelsimple,houses_new)\n",
    "predictionsimple\n",
    "print(predictionsimple)\n",
    "predictionsimple =  pd.DataFrame({'ID':predictionsimple.index, 'Price':predictionsimple})\n",
    "predictionsimple.to_csv('my_submission_simple.csv', index=False)\n",
    "\n",
    "predictionmultiple = my_prediction(modelmultiple,houses_new)\n",
    "print(predictionmultiple)\n",
    "predictionmultiple =  pd.DataFrame({'ID':predictionmultiple.index, 'Price':predictionmultiple})\n",
    "predictionmultiple.to_csv('my_submission_multi.csv', index=False)\n",
    "\n",
    "predictionrsquaredforward = my_prediction(my_regression(houses,selectedbyrsquared,0),houses_new)\n",
    "predictionrsquaredforward =  pd.DataFrame({'ID':predictionrsquaredforward.index, 'Price':predictionrsquaredforward})\n",
    "predictionrsquaredforward.to_csv('my_submission_rsquared.csv', index=False)\n",
    "\n",
    "predictiongenforward = my_prediction(my_regression(houses,selectedbygen,0),houses_new)\n",
    "predictiongenforward =  pd.DataFrame({'ID':predictiongenforward.index, 'Price':predictiongenforward})\n",
    "predictiongenforward.to_csv('my_submission_gen.csv', index=False)\n",
    "\n",
    "predictionpvalueforward = my_prediction(my_regression(houses,selectedbypvalues,0),houses_new)\n",
    "predictionpvalueforward =  pd.DataFrame({'ID':predictionpvalueforward.index, 'Price':predictionpvalueforward})\n",
    "predictionpvalueforward.to_csv('my_submission_pvalues.csv', index=False)\n",
    "\n",
    "polyprediction=my_prediction(modelpolynomial,poly_houses_new)\n",
    "polyprediction =  pd.DataFrame({'ID':polyprediction.index, 'Price':polyprediction})\n",
    "polyprediction.to_csv('my_submission_poly.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
